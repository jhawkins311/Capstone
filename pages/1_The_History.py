import streamlit as st

# Page title
st.title("Chapter 1: The History")

# Introduction
st.markdown("""

Welcome to Chapter 1: Origins of the Synthetic Data 101 course!

In this chapter, youâ€™ll explore the history and motivation behind synthetic data. Youâ€™ll learn how it began as a solution to privacy concerns in public datasetsâ€”and how it evolved into a key tool for secure, scalable, and ethical data science.

---

### ğŸ“œ The Origin
Synthetic data may seem like a cutting-edge innovation, but its origins stretch back over 30 years.

In 1993, the U.S. Census Bureau faced a dilemma: how could it share detailed individual survey responses for research and policy purposes **without revealing private identities**? 

The answer came from statistician Donald Rubin, who proposed using **statistical models** to generate synthetic data that mirrored the real datasetâ€”without including any actual records. 
His approach protected privacy while enabling public access. 

This laid the foundation for todayâ€™s synthetic data movement.

---

### ğŸ“‰ The Two Major Roadblocks

As the demand for data grew, two major challenges began to limit access to useful datasets:

- **Data Scarcity**  
  The shortage of high-quality, labeled, or representative datasets.

- **Data Security**  
  The legal, ethical, and technical risks of sharing sensitive public or private data.

These roadblocks persist todayâ€”especially in regulated industries like **healthcare, finance, and education**.

---

### ğŸ”§ Possible Solutions

To overcome these challenges, researchers and organizations adopted a new set of strategies:

- **Collaborating** â€“ Partnering with data owners to access secure datasets  
- **Cleaning** â€“ Preparing datasets to be usable for AI and modeling  
- **Simplifying** â€“ Using simpler models to create smaller, safe datasets  
- **Generating** â€“ Creating new datasets that mimic the original, but pose no privacy risk

---

### ğŸ§  Enter: Synthetic Data

> Synthetic data refers to **artificially generated datasets** that mimic real-world data in **structure and behavior**â€”while **avoiding any privacy risks**.

Modern tools like **MostlyAI**, **Gretel**, and **YData** generate synthetic records that can be used to:
- Diagnose medical conditions faster  
- Test financial systems under stress  
- Train autonomous vehicles without physical crashes

---

### ğŸ“ How Do We Evaluate Synthetic Data?

Synthetic datasets are evaluated based on three core criteria:

1. **Accuracy** â€“ How well does the synthetic data reflect the patterns and relationships in the original?
2. **Utility** â€“ Can machine learning models trained on the synthetic data produce accurate results?
3. **Privacy** â€“ How unlikely is it that anyone could re-identify individuals from the synthetic dataset?

A 2025 study by JP Morgan showed that high-quality synthetic data could power fraud detection modelsâ€”**without violating privacy regulations**.

---

### ğŸ§ª Next Up: The Models

Now that you understand the origin story and purpose of synthetic data, letâ€™s explore the **four foundational models** that generate it.
""")
